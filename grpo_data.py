import json
import random
import torch
from typing import List, Dict, Callable
from reward_utils import qa_reward


def f1_reward(generated: str, reference: str, query: str) -> float:
    """Wrapper that ignores ``query`` when computing F1."""
    return qa_reward(generated, reference)


def load_qa_dataset(path: str) -> List[Dict[str, str]]:
    """Load a QA dataset where each record has 'query' and 'answer'."""
    data = []
    if path.endswith(".jsonl"):
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                obj = json.loads(line)
                data.append({'query': obj['query'], 'answer': obj['answer']})
    elif path.endswith('.json'):
        with open(path, 'r', encoding='utf-8') as f:
            for obj in json.load(f):
                data.append({'query': obj['query'], 'answer': obj['answer']})
    else:
        raise ValueError('Unsupported dataset format')
    return data


def load_math_dataset(split: str = "test[:500]") -> List[Dict[str, str]]:
    """Load the MATH benchmark via the :mod:`datasets` library."""
    from datasets import load_dataset

    ds = load_dataset("hendrycks/math", split=split)
    return [{"query": x["problem"], "answer": x["solution"]} for x in ds]


def load_gsm8k_dataset(split: str = "test") -> List[Dict[str, str]]:
    """Load the GSM8K dataset."""
    from datasets import load_dataset

    ds = load_dataset("openai/gsm8k", "main", split=split)
    return [{"query": x["question"], "answer": x["answer"]} for x in ds]


def load_minerva_math_dataset(split: str = "test") -> List[Dict[str, str]]:
    """Load the Minerva Math dataset used in the paper."""
    from datasets import load_dataset

    ds = load_dataset("knoveleng/Minerva-Math", split=split)
    return [{"query": x["problem"], "answer": x["solution"]} for x in ds]


def load_olympiadbench_dataset(split: str = "test") -> List[Dict[str, str]]:
    """Load the OlympiadBench dataset."""
    from datasets import load_dataset

    ds = load_dataset("lmms-lab/OlympiadBench", split=split)
    return [{"query": x["problem"], "answer": x["solution"]} for x in ds]


def pad_sequences(seqs: List[List[int]], pad_id: int) -> torch.Tensor:
    max_len = max(len(s) for s in seqs)
    tensor = torch.full((len(seqs), max_len), pad_id, dtype=torch.long)
    for i, s in enumerate(seqs):
        tensor[i, :len(s)] = torch.tensor(s, dtype=torch.long)
    return tensor


def build_grpo_batch(
    samples: List[Dict[str, str]],
    tokenizer,
    model,
    group_size: int,
    max_length: int,
    reward_fn: Callable[[str, str, str], float] = f1_reward,
) -> (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor):
    """Generate candidate responses and compute rewards."""
    q_tokens = [tokenizer.encode(s['query'], add_special_tokens=False) for s in samples]
    answers = [s['answer'] for s in samples]
    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id
    queries = pad_sequences(q_tokens, pad_id)
    q_lens = torch.tensor([len(q) for q in q_tokens], dtype=torch.long)

    B = len(samples)
    responses = []
    lengths = []
    rewards = []
    for i in range(B):
        q = q_tokens[i]
        inp = torch.tensor([q], dtype=torch.long)
        grp_resp = []
        grp_len = []
        grp_rew = []
        for _ in range(group_size):
            out = model.generate(inp, max_length=len(q) + max_length, do_sample=True)
            resp = out[0, len(q):].tolist()
            grp_resp.append(resp)
            grp_len.append(len(resp))
            gen_text = tokenizer.decode(resp)
            grp_rew.append(reward_fn(gen_text, answers[i], samples[i]["query"]))
        responses.append(grp_resp)
        lengths.append(grp_len)
        rewards.append(grp_rew)

    max_resp_len = max(max(l) for l in lengths)
    resp_tensor = torch.full((B, group_size, max_resp_len), pad_id, dtype=torch.long)
    len_tensor = torch.zeros(B, group_size, dtype=torch.long)
    for b in range(B):
        for g in range(group_size):
            seq = responses[b][g]
            resp_tensor[b, g, :len(seq)] = torch.tensor(seq, dtype=torch.long)
            len_tensor[b, g] = len(seq)
    reward_tensor = torch.tensor(rewards, dtype=torch.float)
    return queries, q_lens, resp_tensor, len_tensor, reward_tensor


def construct_second_pass_input(
    query_tokens: torch.Tensor,
    output_tokens: torch.Tensor,
    guidance_tokens: torch.Tensor,
    sep_token: int | None = None,
) -> (torch.Tensor, int):
    """Combine query, first-pass output and guiding prompt tokens.

    Parameters
    ----------
    query_tokens : torch.Tensor
        Tokens representing the original query.
    output_tokens : torch.Tensor
        Tokens generated by the first GRPO layer.
    guidance_tokens : torch.Tensor
        Tokens for the guiding prompt used during self-correction.
    sep_token : int | None, optional
        Optional separator token inserted between the first-pass output and
        the guiding prompt.
    """

    parts = [query_tokens.view(-1), output_tokens.view(-1)]
    if sep_token is not None:
        parts.append(torch.tensor([int(sep_token)], dtype=torch.long))
    parts.append(guidance_tokens.view(-1))
    combined = torch.cat(parts)
    return combined, combined.numel()

